diff --git a/README.md b/README.md
index e5e1ed9f940ebb4fd161c4c23bbef8e327452dde..277c7781aaf1d3d31f3778b5e277e5e5cdae10c3 100644
--- a/README.md
+++ b/README.md
@@ -1 +1,30 @@
-Leap of Thought (LoT)
\ No newline at end of file
+# Leap of Thought (LoT)
+
+This project explores **Leap‑of‑Thought** reasoning with language models.  A model produces a normal chain‑of‑thought and, once a criterion is met, leaps to a new idea using the special `<leap>` token.
+
+## Repository layout
+- `input_prompt/` – utilities for LoT‑1 style prompt experiments
+- `reasoning_output/` – implementation of LoT‑2 with reasoning‑wise perplexity (RWP)
+- `dataset/` – small helpers to stream GSM8K examples
+- `model_provider.py` – wraps local HuggingFace models or OpenAI‑compatible APIs
+- `config.py` – runtime settings
+
+## Quick sanity check
+`reasoning_output/quick_test.py` performs a simple cut‑and‑paste leap.  It does **not** use RWP but is handy to verify your model setup.
+
+```bash
+python -m reasoning_output.quick_test --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --num_samples 1
+```
+
+## Criterion‑based evaluation
+The real LoT‑2 pipeline monitors reasoning‑wise perplexity as it generates.  When perplexity spikes, it injects a `<leap>` snippet and continues.  Try it interactively via `criterion_test.py` or run a small dataset evaluation:
+
+```bash
+# Run a handful of samples and show accuracy
+LOT_PROVIDER=local python -m reasoning_output.criterion_test --num_samples 3
+
+# Evaluate a JSONL dataset (see `input_prompt/data/gsm8k_demo.jsonl`)
+LOT_PROVIDER=local python -m reasoning_output.main --fewshot
+```
+
+Evaluation summaries and detailed records are stored under `logs/`.
diff --git a/reasoning_output/criterion_test.py b/reasoning_output/criterion_test.py
new file mode 100644
index 0000000000000000000000000000000000000000..0015cb58471eb0927c92cd1408aa3f475d5e91cc
--- /dev/null
+++ b/reasoning_output/criterion_test.py
@@ -0,0 +1,61 @@
+from __future__ import annotations
+
+"""Demo script exercising the RWP-based leap insertion pipeline (LoT-2)."""
+
+import argparse
+from tqdm import tqdm
+
+from dataset.gsm8k_loader import GSM8KLoader
+from reasoning_output.src.generator import LeapGenerator
+from reasoning_output.src.utils import extract_answer, normalize_answer
+from utils import set_seed
+
+
+def main() -> None:
+    parser = argparse.ArgumentParser(description="Run a few examples with LoT-2")
+    parser.add_argument("--num_samples", type=int, default=5,
+                        help="How many GSM8K samples to run")
+    parser.add_argument("--fewshot", action="store_true",
+                        help="Use few-shot prompting")
+    args = parser.parse_args()
+
+    set_seed()
+    loader = GSM8KLoader(split="train", num_samples=args.num_samples)
+    gen = LeapGenerator()
+
+    correct_first = 0
+    correct_final = 0
+
+    for sample in tqdm(loader, total=args.num_samples, desc="Evaluating"):
+        q = sample["question"]
+        gold = sample["answers"]
+        rec = gen.generate(q, fewshot=args.fewshot)
+
+        ans_first = extract_answer(rec["first_answer"])
+        ans_final = extract_answer(rec.get("leap_answer") or rec["first_answer"])
+
+        norm_first = normalize_answer(ans_first)
+        norm_final = normalize_answer(ans_final)
+        norm_gold = [normalize_answer(a) for a in gold]
+
+        if any(norm_first.lower() == g.lower() for g in norm_gold):
+            correct_first += 1
+        if any(norm_final.lower() == g.lower() for g in norm_gold):
+            correct_final += 1
+
+        print("\n" + "=" * 80)
+        print("Question:", q)
+        print("\nFirst pass:\n", rec["first_answer"])
+        if rec["trigger_leap"]:
+            print("\nLeap pass:\n", rec.get("leap_answer", ""))
+        else:
+            print("\nNo leap triggered.")
+        print("Predicted answer:", ans_final)
+
+    total = args.num_samples
+    print("\nAccuracy without leap:", correct_first / total if total else 0)
+    print("Accuracy with leap:", correct_final / total if total else 0)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/reasoning_output/quick_test.py b/reasoning_output/quick_test.py
index 9c8bfad750a806b0fe0119a0cca980b01e195b2c..f987fed7ebcb2db3b2b851ce67be69738b6d8ada 100644
--- a/reasoning_output/quick_test.py
+++ b/reasoning_output/quick_test.py
@@ -1,51 +1,51 @@
 from __future__ import annotations
 
 """
 Quick sanity-check for the "reasoning-output" phase using a **local
 Hugging Face chat model**.
 
 Example:
     python -m reasoning_output.quick_test \
         --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
         --num_samples 1 \
         --max-new 2048
 """
 
 import argparse
 import re
 import sys
 import torch
 from typing import List
 
 import torch.nn.functional as F
 from transformers import (
     AutoTokenizer,
     AutoModelForCausalLM,
     DynamicCache,            # HF's dynamic cache implementation
 )
-from reasoning_output.perplexity import split_sentences
+from reasoning_output.src.perplexity import split_sentences
 from input_prompt.src.prompt_engine import build_plain_prompt
 import json
 from tqdm import tqdm
 from utils import set_seed
 from dataset.gsm8k_loader import GSM8KLoader
 
 #───────────────────────────────────────────────────────────────────────────────
 #  Helpers
 #───────────────────────────────────────────────────────────────────────────────
 
 DEVICE = None  # will be set once the model is loaded
 DTYPE  = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
 ANSWER_RE = re.compile(r"\*\*Answer:\*\*\s*(.+)", re.I)
 set_seed()
 
 def load_model(repo_or_path: str):
     """
     Load tokenizer and model from Hugging Face, add <leap> tokens.
     """
     global DEVICE
 
     # Load tokenizer, add special <leap> tokens if not present
     tok = AutoTokenizer.from_pretrained(repo_or_path, use_fast=True)
     SPECIAL_TOKENS = ["<leap>", "</leap>"]
     added = tok.add_special_tokens({"additional_special_tokens": SPECIAL_TOKENS})
diff --git a/reasoning_output/src/criteria.py b/reasoning_output/src/criteria.py
index 283d9a2cb9a9e8adc25a8b86708ee83954375115..9bf59dd52fc4aad22119864934db35bcf7ea80b6 100644
--- a/reasoning_output/src/criteria.py
+++ b/reasoning_output/src/criteria.py
@@ -1,13 +1,13 @@
 """Criterion deciding whether to inject <leap>…</leap>."""
 from __future__ import annotations
 
 from config import settings
-from reasoning_output.perplexity import split_sentences, reasoning_wise_perplexity
+from reasoning_output.src.perplexity import split_sentences, reasoning_wise_perplexity
 from model_provider import BaseModel
 
 
 def should_trigger_leap(rationale: str, model: BaseModel) -> bool:
     """Return **True** if RWP exceeds the configured threshold."""
     sentences = split_sentences(rationale)
     rwp = reasoning_wise_perplexity(sentences, model)
     return rwp >= settings.rwp_threshold
\ No newline at end of file
diff --git a/reasoning_output/src/evaluator.py b/reasoning_output/src/evaluator.py
index 7d21c9a64d0f6187adf42f66bcf043ce288a54f2..c462220f73937a6dd5dc48ca5b0bd3cc22dd8497 100644
--- a/reasoning_output/src/evaluator.py
+++ b/reasoning_output/src/evaluator.py
@@ -1,36 +1,31 @@
-"""Light‑weight evaluator mirroring the original version but with leap logic."""
+"""Light-weight evaluator for the LoT-2 pipeline."""
 from __future__ import annotations
 
 import json
 from pathlib import Path
 from tqdm import tqdm
 
-from reasoning_output.generator import LeapGenerator, dump_record
-import re
+from reasoning_output.src.generator import LeapGenerator, dump_record
+from reasoning_output.src.utils import extract_answer, normalize_answer
 
-ANSWER_RE = re.compile(r"\*\*Answer:\*\*\s*(.+)", re.I)
-
-def _extract_answer(text: str):
-    m = ANSWER_RE.search(text)
-    return m.group(1).strip() if m else None
 
 def evaluate(dataset_path: str, *, fewshot: bool = False):
     gen = LeapGenerator()
     records = []
-    with open(dataset_path) as f:
+    with open(dataset_path, "r", encoding="utf-8") as f:
         data = [json.loads(l) for l in f]
 
     for sample in tqdm(data, desc="Evaluating-with-RWP"):
         rec = gen.generate(sample["question"], fewshot=fewshot)
-        pred = _extract_answer(rec.get("leap_answer") or rec["first_answer"])
-        correct = pred in sample["answers"]
-        rec["pred"] = pred
-        rec["gold"] = sample["answers"]
-        rec["correct"] = correct
+        pred_raw = extract_answer(rec.get("leap_answer") or rec["first_answer"])
+        pred = normalize_answer(pred_raw)
+        gold_norm = [normalize_answer(a) for a in sample["answers"]]
+        correct = any(pred.lower() == g.lower() for g in gold_norm)
+        rec.update({"pred": pred_raw, "gold": sample["answers"], "correct": correct})
         records.append(rec)
         dump_record(rec)
 
     accuracy = sum(r["correct"] for r in records) / len(records)
     summary = {"accuracy": accuracy, "n": len(records)}
     Path("logs/reasoning_output_summary.json").write_text(json.dumps(summary, indent=2))
-    return summary, records
\ No newline at end of file
+    return summary, records
diff --git a/reasoning_output/src/generator.py b/reasoning_output/src/generator.py
index 09fb6736ed55ca9f75a79195c1021f58d5656158..ee10aa9b007eeb7e2f672e41ab34c1547374b8a4 100644
--- a/reasoning_output/src/generator.py
+++ b/reasoning_output/src/generator.py
@@ -1,59 +1,131 @@
-"""Two-pass generation pipeline (baseline → criterion → optional leap)."""
 from __future__ import annotations
 
-from pathlib import Path
+"""Generation utilities for LoT-2 using a token-by-token loop."""
+
 import json
 from datetime import datetime
+from pathlib import Path
+from typing import List
+
+import torch
+import torch.nn.functional as F
+from transformers import AutoModelForCausalLM, AutoTokenizer
 
 from config import settings
-from input_prompt.src.prompt_engine import build_prompt
-from model_provider import get_model
-from reasoning_output.criteria import should_trigger_leap
+from input_prompt.src.prompt_engine import build_plain_prompt
+import math
+from reasoning_output.src.perplexity import sentence_perplexity
 
-# We reuse ANSWER_RE from the old evaluator so users can diff easily
-import re
-ANSWER_RE = re.compile(r"\*\*Answer:\*\*\s*(.+)", re.I)
+from model_provider import get_model, BaseModel
 
 
 class LeapGenerator:
+    """Generate baseline reasoning and optionally a leap extension."""
+
     def __init__(self):
-        self.model = get_model()
+        self.base_model: BaseModel = get_model()
+        if not hasattr(self.base_model, "model"):
+            raise RuntimeError("LeapGenerator currently requires a local HF model")
+        self.mdl: AutoModelForCausalLM = self.base_model.model
+        self.tok: AutoTokenizer = self.base_model.tokenizer
+        self.device = next(self.mdl.parameters()).device
+
+    def _top_p_sample(self, logits: torch.Tensor, temperature: float, top_p: float) -> int:
+        logits = logits / temperature
+        probs = F.softmax(logits, dim=-1)
+        sorted_probs, sorted_idx = torch.sort(probs, descending=True)
+        cumsum = torch.cumsum(sorted_probs, dim=-1)
+        mask = cumsum - sorted_probs > top_p
+        sorted_probs[mask] = 0.0
+        sorted_probs = sorted_probs / sorted_probs.sum()
+        next_token = torch.multinomial(sorted_probs, 1)
+        return sorted_idx[next_token].item()
+
 
-    def _call(self, messages):
-        resp = self.model.chat(messages, max_tokens=settings.max_tokens)
-        content = resp["choices"][0]["message"]["content"]
-        return content, resp["usage"]
+    def generate(self, question: str, fewshot: bool = False) -> dict:
+        """Generate reasoning and dynamically insert a <leap> once RWP is high."""
 
-    def generate(self, question: str, fewshot: bool = False):
-        # ── Pass‑1  (no leap) ──────────────────────────
-        msgs = build_prompt(question, leap=False, fewshot=fewshot)
-        first_content, first_usage = self._call(msgs)
+        prompt = build_plain_prompt(question=question, fewshot=fewshot, leap=False)
+        prompt_ids = self.tok(prompt, return_tensors="pt", add_special_tokens=True).input_ids.to(self.device)
 
-        # Decide whether to trigger leap
-        need_leap = should_trigger_leap(first_content, self.model)
+        # --- first pass: generate until RWP exceeds threshold ---
+        out = self.mdl(input_ids=prompt_ids, use_cache=True)
+        cache = out.past_key_values
+        last_logits = out.logits[:, -1, :]
+
+        gen_ids: List[int] = []
+        sent_start = 0
+        context = ""
+        pps: List[float] = []
+        pivot_token = None
+
+        for _ in range(settings.max_tokens):
+            nxt = self._top_p_sample(last_logits[0], settings.temperature, 0.9)
+            if nxt == self.tok.eos_token_id:
+                break
+            gen_ids.append(nxt)
+            nxt_tensor = torch.tensor([[nxt]], device=self.device)
+            out = self.mdl(input_ids=nxt_tensor, past_key_values=cache, use_cache=True)
+            cache = out.past_key_values
+            last_logits = out.logits[:, -1, :]
+
+            text_piece = self.tok.decode(gen_ids[sent_start:], skip_special_tokens=False)
+            if text_piece.endswith(('.', '!', '?')):
+                sent = text_piece
+                pp = sentence_perplexity(sent, context, self.base_model)
+                pps.append(pp)
+                context += sent + " "
+                sent_start = len(gen_ids)
+                window = pps[-settings.window:]
+                rwp = math.prod(window) ** (1 / len(window))
+                if rwp >= settings.rwp_threshold:
+                    pivot_token = len(gen_ids) - len(self.tok(sent, add_special_tokens=False).input_ids)
+                    break
+
+        normal_ids = gen_ids[:sent_start]
+        normal_text = self.tok.decode(normal_ids, skip_special_tokens=False)
 
         record = {
             "question": question,
-            "first_answer": first_content,
-            "first_usage": first_usage,
-            "trigger_leap": need_leap,
+            "first_answer": normal_text,
+            "trigger_leap": pivot_token is not None,
         }
+        if pivot_token is None:
+            return record
+
+        # --- run model up to prefix ---
+        prefix_ids = prompt_ids[0].tolist() + normal_ids
+        prefix_tensor = torch.tensor([prefix_ids], device=self.device)
+        out = self.mdl(input_ids=prefix_tensor, use_cache=True)
+        cache = out.past_key_values
+        last_logits = out.logits[:, -1, :]
+
+        # --- inject leap and continue generation ---
+        leap_text = "<leap>I have a new idea to make it quick and clever. "
+        leap_ids = self.tok(leap_text, add_special_tokens=False).input_ids
+        leap_tensor = torch.tensor([leap_ids], device=self.device)
+        out = self.mdl(input_ids=leap_tensor, past_key_values=cache, use_cache=True)
+        cache = out.past_key_values
+        last_logits = out.logits[:, -1, :]
 
-        if not need_leap:
-            return record  # good enough
+        lot_ids: List[int] = []
+        for _ in range(settings.max_tokens - len(prefix_ids)):
+            nxt = self._top_p_sample(last_logits[0], settings.temperature, 0.9)
+            if nxt == self.tok.eos_token_id:
+                break
+            lot_ids.append(nxt)
+            new_tensor = torch.tensor([[nxt]], device=self.device)
+            out = self.mdl(input_ids=new_tensor, past_key_values=cache, use_cache=True)
+            cache = out.past_key_values
+            last_logits = out.logits[:, -1, :]
 
-        # ── Pass‑2  (with <leap>) ──────────────────────
-        leap_msgs = build_prompt(question, leap=True, fewshot=fewshot)
-        leap_content, leap_usage = self._call(leap_msgs)
-        record.update({
-            "leap_answer": leap_content,
-            "leap_usage": leap_usage,
-        })
+        lot_text = normal_text + leap_text + self.tok.decode(lot_ids, skip_special_tokens=False)
+        record["leap_answer"] = lot_text
         return record
 
 
-def dump_record(rec):
+def dump_record(rec: dict) -> None:
     ts = datetime.utcnow().isoformat()
     out_path = Path(settings.log_dir) / "reasoning_records.jsonl"
     with out_path.open("a", encoding="utf-8") as f:
-        f.write(json.dumps({"ts": ts, **rec}) + "\n")
\ No newline at end of file
+        f.write(json.dumps({"ts": ts, **rec}) + "\n")
diff --git a/reasoning_output/src/utils.py b/reasoning_output/src/utils.py
new file mode 100644
index 0000000000000000000000000000000000000000..b768ecda846b53dea57a0a9628edfd7ff024c173
--- /dev/null
+++ b/reasoning_output/src/utils.py
@@ -0,0 +1,24 @@
+import re
+
+ANSWER_RE = re.compile(r"\*\*Answer:\*\*\s*(.+)", re.I)
+
+
+def extract_answer(text: str) -> str:
+    """Return the raw answer following the `**Answer:**` marker."""
+    m = ANSWER_RE.search(text)
+    return m.group(1).strip() if m else ""
+
+
+def normalize_answer(ans: str) -> str:
+    """Normalize numeric strings for robust comparison."""
+    ans = ans.strip().strip(" .,!?")
+    ans = ans.lstrip("$€£¥").replace(",", "")
+    if re.fullmatch(r"-?\d+(?:\.\d+)?", ans):
+        try:
+            num = float(ans)
+        except ValueError:
+            return ans
+        if num.is_integer():
+            return str(int(num))
+        ans = ("%f" % num).rstrip("0").rstrip(".")
+    return ans
